{"cve": {"id": "CVE-2024-53054", "sourceIdentifier": "416baaa9-dc9f-4396-8d5f-8c081fb06d67", "published": "2024-11-19T18:15:25.500", "lastModified": "2024-11-22T17:11:42.763", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In the Linux kernel, the following vulnerability has been resolved:\n\ncgroup/bpf: use a dedicated workqueue for cgroup bpf destruction\n\nA hung_task problem shown below was found:\n\nINFO: task kworker/0:0:8 blocked for more than 327 seconds.\n\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nWorkqueue: events cgroup_bpf_release\nCall Trace:\n <TASK>\n __schedule+0x5a2/0x2050\n ? find_held_lock+0x33/0x100\n ? wq_worker_sleeping+0x9e/0xe0\n schedule+0x9f/0x180\n schedule_preempt_disabled+0x25/0x50\n __mutex_lock+0x512/0x740\n ? cgroup_bpf_release+0x1e/0x4d0\n ? cgroup_bpf_release+0xcf/0x4d0\n ? process_scheduled_works+0x161/0x8a0\n ? cgroup_bpf_release+0x1e/0x4d0\n ? mutex_lock_nested+0x2b/0x40\n ? __pfx_delay_tsc+0x10/0x10\n mutex_lock_nested+0x2b/0x40\n cgroup_bpf_release+0xcf/0x4d0\n ? process_scheduled_works+0x161/0x8a0\n ? trace_event_raw_event_workqueue_execute_start+0x64/0xd0\n ? process_scheduled_works+0x161/0x8a0\n process_scheduled_works+0x23a/0x8a0\n worker_thread+0x231/0x5b0\n ? __pfx_worker_thread+0x10/0x10\n kthread+0x14d/0x1c0\n ? __pfx_kthread+0x10/0x10\n ret_from_fork+0x59/0x70\n ? __pfx_kthread+0x10/0x10\n ret_from_fork_asm+0x1b/0x30\n </TASK>\n\nThis issue can be reproduced by the following pressuse test:\n1. A large number of cpuset cgroups are deleted.\n2. Set cpu on and off repeatly.\n3. Set watchdog_thresh repeatly.\nThe scripts can be obtained at LINK mentioned above the signature.\n\nThe reason for this issue is cgroup_mutex and cpu_hotplug_lock are\nacquired in different tasks, which may lead to deadlock.\nIt can lead to a deadlock through the following steps:\n1. A large number of cpusets are deleted asynchronously, which puts a\n   large number of cgroup_bpf_release works into system_wq. The max_active\n   of system_wq is WQ_DFL_ACTIVE(256). Consequently, all active works are\n   cgroup_bpf_release works, and many cgroup_bpf_release works will be put\n   into inactive queue. As illustrated in the diagram, there are 256 (in\n   the acvtive queue) + n (in the inactive queue) works.\n2. Setting watchdog_thresh will hold cpu_hotplug_lock.read and put\n   smp_call_on_cpu work into system_wq. However step 1 has already filled\n   system_wq, 'sscs.work' is put into inactive queue. 'sscs.work' has\n   to wait until the works that were put into the inacvtive queue earlier\n   have executed (n cgroup_bpf_release), so it will be blocked for a while.\n3. Cpu offline requires cpu_hotplug_lock.write, which is blocked by step 2.\n4. Cpusets that were deleted at step 1 put cgroup_release works into\n   cgroup_destroy_wq. They are competing to get cgroup_mutex all the time.\n   When cgroup_metux is acqured by work at css_killed_work_fn, it will\n   call cpuset_css_offline, which needs to acqure cpu_hotplug_lock.read.\n   However, cpuset_css_offline will be blocked for step 3.\n5. At this moment, there are 256 works in active queue that are\n   cgroup_bpf_release, they are attempting to acquire cgroup_mutex, and as\n   a result, all of them are blocked. Consequently, sscs.work can not be\n   executed. Ultimately, this situation leads to four processes being\n   blocked, forming a deadlock.\n\nsystem_wq(step1)\t\tWatchDog(step2)\t\t\tcpu offline(step3)\tcgroup_destroy_wq(step4)\n...\n2000+ cgroups deleted asyn\n256 actives + n inactives\n\t\t\t\t__lockup_detector_reconfigure\n\t\t\t\tP(cpu_hotplug_lock.read)\n\t\t\t\tput sscs.work into system_wq\n256 + n + 1(sscs.work)\nsscs.work wait to be executed\n\t\t\t\twarting sscs.work finish\n\t\t\t\t\t\t\t\tpercpu_down_write\n\t\t\t\t\t\t\t\tP(cpu_hotplug_lock.write)\n\t\t\t\t\t\t\t\t...blocking...\n\t\t\t\t\t\t\t\t\t\t\tcss_killed_work_fn\n\t\t\t\t\t\t\t\t\t\t\tP(cgroup_mutex)\n\t\t\t\t\t\t\t\t\t\t\tcpuset_css_offline\n\t\t\t\t\t\t\t\t\t\t\tP(cpu_hotplug_lock.read)\n\t\t\t\t\t\t\t\t\t\t\t...blocking...\n256 cgroup_bpf_release\nmutex_lock(&cgroup_mutex);\n..blocking...\n\nTo fix the problem, place cgroup_bpf_release works on a dedicated\nworkqueue which can break the loop and solve the problem. System wqs are\nfor misc things which shouldn't create a large number of concurrent work\nitems. If something is going to generate >\n---truncated---"}, {"lang": "es", "value": "En el kernel de Linux, se ha resuelto la siguiente vulnerabilidad: cgroup/bpf: utiliza una cola de trabajo dedicada para la destrucci\u00f3n de bpf de cgroup Se encontr\u00f3 un problema hung_task que se muestra a continuaci\u00f3n: INFORMACI\u00d3N: la tarea kworker/0:0:8 se bloque\u00f3 durante m\u00e1s de 327 segundos. \"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs\" deshabilita este mensaje. Cola de trabajo: eventos cgroup_bpf_release Rastreo de llamadas:  __schedule+0x5a2/0x2050 ? find_held_lock+0x33/0x100 ? wq_worker_sleeping+0x9e/0xe0 schedule+0x9f/0x180 schedule_preempt_disabled+0x25/0x50 __mutex_lock+0x512/0x740 ? cgroup_bpf_release+0x1e/0x4d0 ? cgroup_bpf_release+0xcf/0x4d0 ? proceso_trabajos_programados+0x161/0x8a0 ? cgroup_bpf_release+0x1e/0x4d0 ? mutex_lock_nested+0x2b/0x40 ? __pfx_delay_tsc+0x10/0x10 mutex_lock_nested+0x2b/0x40 cgroup_bpf_release+0xcf/0x4d0 ? proceso_trabajos_programados+0x161/0x8a0 ? trace_event_raw_event_workqueue_execute_start+0x64/0xd0 ? Este problema se puede reproducir con la siguiente prueba pressuse: 1. Se elimina una gran cantidad de cgroups de cpuset. 2. Enciende y apaga la CPU repetidamente. 3. Establece watchdog_thresh repetidamente. Los scripts se pueden obtener en el ENLACE mencionado arriba de la firma. El motivo de este problema es que cgroup_mutex y cpu_hotplug_lock se adquieren en tareas diferentes, lo que puede provocar un bloqueo. Puede provocar un bloqueo mediante los siguientes pasos: 1. Se elimina una gran cantidad de conjuntos de CPU de forma asincr\u00f3nica, lo que coloca una gran cantidad de trabajos cgroup_bpf_release en system_wq. El max_active de system_wq es WQ_DFL_ACTIVE(256). En consecuencia, todos los trabajos activos son trabajos cgroup_bpf_release y muchos trabajos cgroup_bpf_release se colocar\u00e1n en la cola inactiva. Como se ilustra en el diagrama, hay 256 (en la cola activa) + n (en la cola inactiva) trabajos. 2. La configuraci\u00f3n de watchdog_thresh mantendr\u00e1 cpu_hotplug_lock.read y colocar\u00e1 el trabajo smp_call_on_cpu en system_wq. Sin embargo, el paso 1 ya ha llenado system_wq, 'sscs.work' se coloca en la cola inactiva. 'sscs.work' tiene que esperar hasta que los trabajos que se colocaron en la cola inactiva anteriormente se hayan ejecutado (n cgroup_bpf_release), por lo que se bloquear\u00e1 por un tiempo. 3. La CPU sin conexi\u00f3n requiere cpu_hotplug_lock.write, que est\u00e1 bloqueado por el paso 2. 4. Los conjuntos de CPU que se eliminaron en el paso 1 colocan los trabajos de cgroup_release en cgroup_destroy_wq. Est\u00e1n compitiendo para obtener cgroup_mutex todo el tiempo. Cuando cgroup_metux es adquirido por un trabajo en css_killed_work_fn, llamar\u00e1 a cpuset_css_offline, que necesita adquirir cpu_hotplug_lock.read. Sin embargo, cpuset_css_offline se bloquear\u00e1 para el paso 3. 5. En este momento, hay 256 trabajos en la cola activa que son cgroup_bpf_release, est\u00e1n intentando adquirir cgroup_mutex y, como resultado, todos ellos est\u00e1n bloqueados. En consecuencia, no se puede ejecutar sscs.work. En definitiva, esta situaci\u00f3n provoca el bloqueo de cuatro procesos, lo que genera un punto muerto. system_wq(paso1) WatchDog(paso2) cpu offline(paso3) cgroup_destroy_wq(paso4) ... 2000+ cgroups eliminados asyn 256 activos + n inactivos __lockup_detector_reconfigure P(cpu_hotplug_lock.read) poner sscs.work en system_wq 256 + n + 1(sscs.work) sscs.work esperar a ser ejecutado warting sscs.work finalizar percpu_down_write P(cpu_hotplug_lock.write) ...bloqueando... css_killed_work_fn P(cgroup_mutex) cpuset_css_offline P(cpu_hotplug_lock.read) ...bloqueando... 256 cgroup_bpf_release mutex_lock(&amp;cgroup_mutex); ..bloqueo... Para solucionar el problema, coloque los trabajos de cgroup_bpf_release en una cola de trabajo dedicada que pueda romper el bucle y resolver el problema. Las colas de trabajo del sistema son para cosas diversas que no deber\u00edan crear una gran cantidad de elementos de trabajo simult\u00e1neos. Si algo va a generar &gt; ---truncado---"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "baseScore": 5.5, "baseSeverity": "MEDIUM", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-667"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.3", "versionEndExcluding": "6.1.116", "matchCriteriaId": "E50C2CFF-A644-4571-98EF-C7966E95441D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "6.2", "versionEndExcluding": "6.6.60", "matchCriteriaId": "75088E5E-2400-4D20-915F-7A65C55D9CCD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "6.7", "versionEndExcluding": "6.11.7", "matchCriteriaId": "E96F53A4-5E87-4A70-BD9A-BC327828D57F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:6.12:rc1:*:*:*:*:*:*", "matchCriteriaId": "7F361E1D-580F-4A2D-A509-7615F73167A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:6.12:rc2:*:*:*:*:*:*", "matchCriteriaId": "925478D0-3E3D-4E6F-ACD5-09F28D5DF82C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:6.12:rc3:*:*:*:*:*:*", "matchCriteriaId": "3C95E234-D335-4B6C-96BF-E2CEBD8654ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:6.12:rc4:*:*:*:*:*:*", "matchCriteriaId": "E0F717D8-3014-4F84-8086-0124B2111379"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:6.12:rc5:*:*:*:*:*:*", "matchCriteriaId": "24DBE6C7-2AAE-4818-AED2-E131F153D2FA"}]}]}], "references": [{"url": "https://git.kernel.org/stable/c/0d86cd70fc6a7ba18becb52ad8334d5ad3eca530", "source": "416baaa9-dc9f-4396-8d5f-8c081fb06d67", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/117932eea99b729ee5d12783601a4f7f5fd58a23", "source": "416baaa9-dc9f-4396-8d5f-8c081fb06d67", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/6dab3331523ba73db1345d19e6f586dcd5f6efb4", "source": "416baaa9-dc9f-4396-8d5f-8c081fb06d67", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/71f14a9f5c7db72fdbc56e667d4ed42a1a760494", "source": "416baaa9-dc9f-4396-8d5f-8c081fb06d67", "tags": ["Patch"]}]}}